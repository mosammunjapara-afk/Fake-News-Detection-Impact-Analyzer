"""
Indian News Auto-Collector ‚Äî 3 Free Sources
=============================================
Source 1 (Primary)  : GNews API         ‚Äî 100% free, no card, real-time India news
Source 2 (Secondary): NewsData.io       ‚Äî free tier, real-time India news  
Source 3 (Backup)   : NewsAPI.org       ‚Äî existing key (if quota allows)

Why 3 sources:
- If one API hits daily limit ‚Üí others still collect fresh news
- More variety, less duplicates
- Today's news GUARANTEED

Setup:
  1. GNews    ‚Üí https://gnews.io       ‚Üí free signup ‚Üí copy API key ‚Üí add GNEWS_API_KEY in .env
  2. NewsData ‚Üí https://newsdata.io    ‚Üí free signup ‚Üí copy API key ‚Üí add NEWSDATA_API_KEY in .env  
  3. NewsAPI  ‚Üí already in .env as NEWS_API_KEY (existing)
"""

import os
import requests
import time
import hashlib
import threading
import schedule
from datetime import datetime, timedelta
from typing import List, Dict, Optional

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# SOURCE CREDIBILITY SCORES
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
TRUSTED_SOURCES = {
    'The Hindu': 0.9, 'Business Standard': 0.9, 'PTI': 0.88,
    'The Indian Express': 0.88, 'Economic Times': 0.85,
    'The Times of India': 0.85, 'Mint': 0.85, 'ANI': 0.82,
    'NDTV': 0.82, 'Hindustan Times': 0.82, 'India Today': 0.80,
    'News18': 0.78, 'CNBC-TV18': 0.80, 'The Print': 0.78,
    'Scroll.in': 0.78, 'The Wire': 0.75, 'Deccan Herald': 0.78,
    'Tribune India': 0.75, 'Outlook India': 0.75,
}
QUESTIONABLE_SOURCES = {
    'Unknown': 0.3, 'Social Media': 0.2, '[Removed]': 0.0,
}


class IndianNewsCollector:
    """
    Auto-collects Indian news from 3 free sources.
    Each source is tried independently ‚Äî if one fails, others fill in.
    """

    def __init__(self, api_key: str, database, impact_generator=None):
        self.database       = database
        self.impact_generator = impact_generator

        # API Keys ‚Äî read from environment
        self.newsapi_key  = api_key                            # From NEWS_API_KEY in .env
        self.gnews_key    = os.getenv("GNEWS_API_KEY", "")    # New: GNews
        self.newsdata_key = os.getenv("NEWSDATA_API_KEY", "") # New: NewsData.io

        # Endpoints
        self.gnews_url    = "https://gnews.io/api/v4/top-headlines"
        self.newsdata_url = "https://newsdata.io/api/1/news"
        self.newsapi_url  = "https://newsapi.org/v2/top-headlines"
        self.newsapi_everything = "https://newsapi.org/v2/everything"

        self._print_status()

    def _print_status(self):
        print("\nüì° News Collector ‚Äî Source Status:")
        print(f"   {'‚úÖ' if self.gnews_key    else '‚ùå'} GNews API      {'(active)' if self.gnews_key    else '‚Üí get free key: https://gnews.io'}")
        print(f"   {'‚úÖ' if self.newsdata_key else '‚ùå'} NewsData.io    {'(active)' if self.newsdata_key else '‚Üí get free key: https://newsdata.io'}")
        print(f"   {'‚úÖ' if self.newsapi_key  else '‚ùå'} NewsAPI.org    {'(active)' if self.newsapi_key  else '‚Üí missing NEWS_API_KEY'}")
        active = sum([bool(self.gnews_key), bool(self.newsdata_key), bool(self.newsapi_key)])
        print(f"   üìä {active}/3 sources active\n")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # SOURCE 1 ‚Äî GNews API  (RECOMMENDED: best free India news)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def fetch_gnews(self, topic: str = "breaking-news", max_articles: int = 10) -> List[Dict]:
        """
        GNews free plan: 100 requests/day, up to 10 articles/request.
        Topics: breaking-news, world, nation, business, technology,
                entertainment, sports, science, health
        """
        if not self.gnews_key:
            return []

        params = {
            "token":    self.gnews_key,
            "country":  "in",
            "lang":     "en",
            "topic":    topic,
            "max":      max_articles,
            "sortby":   "publishedAt",    # Always newest first
        }

        try:
            r = requests.get(self.gnews_url, params=params, timeout=15)

            if r.status_code == 200:
                data = r.json()
                raw = data.get("articles", [])
                articles = [self._normalize_gnews(a) for a in raw if a.get("title")]
                print(f"  ‚úÖ GNews [{topic}]: {len(articles)} articles")
                return articles

            elif r.status_code == 403:
                print(f"  ‚ùå GNews: Invalid API key")
            elif r.status_code == 429:
                print(f"  ‚ö†Ô∏è  GNews: Daily limit reached (100 req/day on free plan)")
            else:
                print(f"  ‚ùå GNews HTTP {r.status_code} [{topic}]")

        except Exception as e:
            print(f"  ‚ùå GNews error [{topic}]: {e}")

        return []

    def _normalize_gnews(self, a: Dict) -> Dict:
        """Normalize GNews article to standard format"""
        source_name = a.get("source", {}).get("name", "GNews")
        return {
            "title":       a.get("title", ""),
            "description": a.get("description", ""),
            "content":     a.get("content", ""),
            "url":         a.get("url", ""),
            "publishedAt": a.get("publishedAt", ""),
            "source":      {"name": source_name},
        }

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # SOURCE 2 ‚Äî NewsData.io
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def fetch_newsdata(self, category: str = "top") -> List[Dict]:
        """
        NewsData.io free plan: 200 results/request, real-time.
        """
        if not self.newsdata_key:
            return []

        params = {
            "apikey":   self.newsdata_key,
            "country":  "in",
            "language": "en",
            "category": category,
        }

        try:
            r = requests.get(self.newsdata_url, params=params, timeout=15)

            if r.status_code == 200:
                data = r.json()
                if data.get("status") == "success":
                    raw = data.get("results", [])
                    articles = [self._normalize_newsdata(a) for a in raw if a.get("title")]
                    print(f"  ‚úÖ NewsData.io [{category}]: {len(articles)} articles")
                    return articles
                else:
                    print(f"  ‚ö†Ô∏è  NewsData.io: {data.get('message','Unknown error')}")
            elif r.status_code == 422:
                print(f"  ‚ö†Ô∏è  NewsData.io: Invalid category '{category}'")
            elif r.status_code == 429:
                print(f"  ‚ö†Ô∏è  NewsData.io: Rate limit hit")
                time.sleep(30)
            else:
                print(f"  ‚ùå NewsData.io HTTP {r.status_code}")

        except Exception as e:
            print(f"  ‚ùå NewsData.io error: {e}")

        return []

    def _normalize_newsdata(self, a: Dict) -> Dict:
        source_name = a.get("source_id", "newsdata").replace("-", " ").title()
        return {
            "title":       a.get("title", ""),
            "description": a.get("description") or a.get("content", ""),
            "content":     a.get("content", ""),
            "url":         a.get("link", ""),
            "publishedAt": a.get("pubDate", ""),
            "source":      {"name": source_name},
        }

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # SOURCE 3 ‚Äî NewsAPI.org  (existing, use as backup)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def fetch_newsapi(self, category: str = "general") -> List[Dict]:
        """NewsAPI.org ‚Äî used as backup. Free plan has 24h delay limitation."""
        if not self.newsapi_key:
            return []

        # Try top-headlines first
        params = {
            "country":  "in",
            "category": category,
            "pageSize": 20,
            "apiKey":   self.newsapi_key,
        }
        try:
            r = requests.get(self.newsapi_url, params=params, timeout=10)
            if r.status_code == 200:
                data = r.json()
                if data.get("status") == "ok":
                    articles = data.get("articles", [])
                    print(f"  ‚úÖ NewsAPI [{category}]: {len(articles)} articles")
                    return articles
            elif r.status_code in [426, 401]:
                # Plan limitation ‚Äî try /everything with last 24h filter
                return self._newsapi_everything(category)
            elif r.status_code == 429:
                print(f"  ‚ö†Ô∏è  NewsAPI: Rate limit hit")
                time.sleep(30)
            else:
                print(f"  ‚ö†Ô∏è  NewsAPI HTTP {r.status_code} ‚Äî trying /everything")
                return self._newsapi_everything(category)
        except Exception as e:
            print(f"  ‚ùå NewsAPI error: {e}")
        return []

    def _newsapi_everything(self, category: str) -> List[Dict]:
        queries = {
            "general": "india today", "business": "india economy finance",
            "technology": "india tech startup AI", "health": "india health",
            "sports": "india cricket IPL", "entertainment": "bollywood india",
        }
        q = queries.get(category, "india news")
        from_dt = (datetime.utcnow() - timedelta(hours=24)).strftime('%Y-%m-%dT%H:%M:%S')

        try:
            r = requests.get(self.newsapi_everything, params={
                "q": q, "from": from_dt, "language": "en",
                "sortBy": "publishedAt", "pageSize": 20,
                "apiKey": self.newsapi_key,
            }, timeout=10)
            if r.status_code == 200:
                data = r.json()
                if data.get("status") == "ok":
                    articles = data.get("articles", [])
                    print(f"  ‚úÖ NewsAPI /everything [{category}]: {len(articles)} articles")
                    return articles
            elif r.status_code == 429:
                print(f"  ‚ö†Ô∏è  NewsAPI /everything: Rate limit")
                time.sleep(30)
        except Exception as e:
            print(f"  ‚ùå NewsAPI /everything error: {e}")
        return []

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # DUPLICATE DETECTION
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _content_hash(self, title: str, published_at: str) -> str:
        """Hash of normalized title + date ‚Äî catches mirrors and syndicated copies"""
        clean = ''.join(c.lower() for c in title if c.isalnum() or c.isspace()).strip()
        date  = published_at[:10] if published_at else ""
        return hashlib.md5(f"{clean}|{date}".encode()).hexdigest()

    def _is_duplicate(self, url: str, title: str, published_at: str) -> bool:
        if self.database.is_url_exists(url):
            return True
        if self.database.is_content_hash_exists(self._content_hash(title, published_at)):
            return True
        return False

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # VERIFICATION
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _source_trust(self, name: str) -> float:
        if name in TRUSTED_SOURCES:    return TRUSTED_SOURCES[name]
        if name in QUESTIONABLE_SOURCES: return QUESTIONABLE_SOURCES[name]
        return 0.5

    def verify_article(self, article: Dict, model, vectorizer) -> Dict:
        from utils.preprocess import clean_text, short_text
        from utils.claim_type import detect_claim_type
        from utils.wiki_verify import wikipedia_verify

        title  = article.get("title", "")
        desc   = article.get("description", "") or ""
        body   = article.get("content", "") or ""
        source = article.get("source", {}).get("name", "Unknown")

        if not title or title == "[Removed]":
            return {"result": "‚ö†Ô∏è NEEDS FACT CHECKING", "confidence": 0.0,
                    "explanation": "Article removed or unavailable"}

        text  = f"{title} {desc} {body}"
        trust = self._source_trust(source)

        if wikipedia_verify(text):
            return {"result": "üü¢ VERIFIED FACT", "confidence": 95.0,
                    "explanation": "Verified using Wikipedia"}

        if detect_claim_type(text) == "HISTORICAL":
            return {"result": "üü¢ VERIFIED FACT", "confidence": 95.0,
                    "explanation": "Historical fact"}

        processed = clean_text(short_text(text))
        vec   = vectorizer.transform([processed])
        probs = model.predict_proba(vec)[0]
        ml_conf  = round(max(probs) * 100, 2)
        ml_label = model.classes_[probs.argmax()]
        combined = (ml_conf * 0.7) + (trust * 100 * 0.3)

        if combined < 60:
            return {"result": "‚ö†Ô∏è NEEDS FACT CHECKING", "confidence": round(combined, 2),
                    "explanation": f"Low confidence (Source: {source})"}
        elif ml_label == 1:
            return {"result": "üü¢ REAL NEWS", "confidence": round(combined, 2),
                    "explanation": f"ML prediction (trust: {trust:.2f})"}
        else:
            if trust > 0.8 and combined < 85:
                return {"result": "‚ö†Ô∏è NEEDS FACT CHECKING", "confidence": round(combined, 2),
                        "explanation": "Trusted source flagged ‚Äî needs manual review"}
            return {"result": "üî¥ FAKE NEWS", "confidence": round(combined, 2),
                    "explanation": f"ML prediction (trust: {trust:.2f})"}

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # AI IMPACTS
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _gen_impacts(self, title: str, desc: str, is_fake: bool):
        if not self.impact_generator:
            return None
        try:
            fn = (self.impact_generator.generate_fake_news_impact if is_fake
                  else self.impact_generator.generate_real_news_impact)
            impacts = fn(title, desc)
            print(f"   ü§ñ AI impacts generated")
            return impacts
        except Exception as e:
            print(f"   ‚ö†Ô∏è  AI impact failed: {e}")
            return None

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PROCESS & STORE
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def process_and_store(self, articles: List[Dict], model, vectorizer):
        stored = fake = dupes = 0

        # Remove URL duplicates within this batch
        seen, unique = set(), []
        for a in articles:
            u = a.get("url", "")
            if u and u not in seen:
                unique.append(a)
                seen.add(u)

        print(f"\nüì¶ Processing {len(unique)} unique articles from batch...")

        for a in unique:
            try:
                title  = a.get("title", "")
                desc   = a.get("description", "") or ""
                source = a.get("source", {}).get("name", "Unknown")
                url    = a.get("url", "")
                pub_at = a.get("publishedAt", "")

                if not title or title == "[Removed]" or not url:
                    continue

                if self._is_duplicate(url, title, pub_at):
                    dupes += 1
                    continue

                chash        = self._content_hash(title, pub_at)
                verification = self.verify_article(a, model, vectorizer)
                is_fake      = "FAKE" in verification["result"]
                impacts      = self._gen_impacts(title, desc, is_fake)

                ok = self.database.add_auto_collected_news(
                    headline=title, description=desc, source=source,
                    url=url, published_at=pub_at,
                    result=verification["result"],
                    confidence=verification["confidence"],
                    ai_impacts=impacts, content_hash=chash,
                )

                if ok:
                    stored += 1
                    tag = "üö® FAKE" if is_fake else "‚úÖ REAL"
                    print(f"  {tag}: {title[:70]}...")
                    fake += is_fake
                else:
                    dupes += 1

                time.sleep(1.2 if impacts else 0.3)

            except Exception as e:
                print(f"  ‚ùå Error: {e}")

        self.database.add_collection_stats(
            fetched=len(articles), stored=stored,
            duplicates=dupes, fake=fake,
        )

        print(f"\nüìä Batch result ‚Üí Raw: {len(articles)} | Unique: {len(unique)} | "
              f"NEW: {stored} | Dupes skipped: {dupes} | Fake: {fake}")
        return stored, fake

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # MAIN COLLECTION ‚Äî called every 3 hours by scheduler
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def collect_daily_news(self, model, vectorizer):
        print(f"\n{'='*65}")
        print(f"üóûÔ∏è  Auto Collection ‚Äî {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*65}")

        all_articles = []

        # ‚îÄ‚îÄ SOURCE 1: GNews (best free real-time India news) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        if self.gnews_key:
            print("\nüì° Source 1: GNews API...")
            gnews_topics = [
                "breaking-news", "nation", "business",
                "technology", "sports", "health", "entertainment", "science"
            ]
            for topic in gnews_topics:
                articles = self.fetch_gnews(topic=topic, max_articles=10)
                all_articles.extend(articles)
                time.sleep(1)   # GNews: respect 100 req/day limit
        else:
            print("\n‚ö†Ô∏è  Source 1 SKIPPED ‚Äî add GNEWS_API_KEY to .env")
            print("   üëâ Free signup at https://gnews.io (takes 1 minute)")

        # ‚îÄ‚îÄ SOURCE 2: NewsData.io ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        if self.newsdata_key:
            print("\nüì° Source 2: NewsData.io...")
            for cat in ["top", "politics", "business", "technology", "sports", "health"]:
                articles = self.fetch_newsdata(category=cat)
                all_articles.extend(articles)
                time.sleep(0.8)
        else:
            print("\n‚ö†Ô∏è  Source 2 SKIPPED ‚Äî add NEWSDATA_API_KEY to .env")
            print("   üëâ Free signup at https://newsdata.io (takes 1 minute)")

        # ‚îÄ‚îÄ SOURCE 3: NewsAPI.org (existing key, backup) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        if self.newsapi_key:
            print("\nüì° Source 3: NewsAPI.org (backup)...")
            for cat in ["general", "business", "technology", "health", "sports"]:
                articles = self.fetch_newsapi(category=cat)
                all_articles.extend(articles)
                time.sleep(0.5)
        else:
            print("\n‚ö†Ô∏è  Source 3 SKIPPED ‚Äî NEWS_API_KEY missing in .env")

        print(f"\nüìö Total articles collected across all sources: {len(all_articles)}")

        if not all_articles:
            print("\nüö® ZERO articles collected!")
            print("   Possible reasons:")
            print("   1. All API keys are missing ‚Äî check your .env file")
            print("   2. NewsAPI free plan expired/rate-limited")
            print("   3. Internet connection issue")
            print("\n   üëâ Quick fix: Add GNEWS_API_KEY to .env (free at https://gnews.io)")
            return 0, 0

        stored, fake = self.process_and_store(all_articles, model, vectorizer)
        print(f"\n‚úÖ Collection complete! {stored} new articles stored | {fake} fake detected\n")
        return stored, fake


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# SCHEDULER
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def schedule_news_collection(collector, model, vectorizer):
    def job():
        try:
            print(f"\n‚è∞ Scheduled run at {datetime.now().strftime('%H:%M:%S')}")
            collector.collect_daily_news(model, vectorizer)
        except Exception as e:
            print(f"‚ùå Scheduled job failed: {e}")

    schedule.every(3).hours.do(job)
    for t in ["06:00", "09:00", "12:00", "15:00", "18:00", "21:00"]:
        schedule.every().day.at(t).do(job)

    print("üìÖ Schedule: Every 3h + 6/9/12/15/18/21")

    print("üöÄ Initial collection on startup...")
    job()

    while True:
        schedule.run_pending()
        time.sleep(60)


def start_news_collector_thread(collector, model, vectorizer):
    t = threading.Thread(
        target=schedule_news_collection,
        args=(collector, model, vectorizer),
        daemon=True,
    )
    t.start()
    print("‚úÖ News collector thread started")
